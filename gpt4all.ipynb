{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_path = '/mnt/d/llm_models/gpt4all/gpt4all-lora-quantized-ggml.bin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Path(local_path).parent.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://the-eye.eu/public/AI/models/nomic-ai/gpt4all/gpt4all-lora-quantized-ggml.bin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the gpt4all stream\n",
    "response = requests.get(url, stream=True)\n",
    "# open the file in binary mode, writing the contents of the response to it in chunks.\n",
    "with open(local_path, 'wb') as f:\n",
    "  for chunk in tqdm(response.iter_content(chunk_size=8192)):\n",
    "    if chunk:\n",
    "      f.write(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/ggerganov/llama.cpp.git /mnt/d/llm_models/gpt4all/llama.cpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m /mnt/d/llm_models/gpt4all/llama.cpp/convert.py /mnt/d/llm_models/gpt4all/gpt4all-lora-quantized-ggml.bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model and generate\n",
    "from langchain_community.llms import GPT4All\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.callbacks.base import CallbackManagerMixin\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# few-shot learning, telling the model how to answer.\n",
    "template = \"\"\"\n",
    "  Question: {question}\n",
    "  Answer: Let's think step by step.\n",
    "\"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=['question'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [StreamingStdOutCallbackHandler()]\n",
    "llm = GPT4All(model=\"/mnt/d/llm_models/gpt4all/gpt4all-falcon-newbpe-q4_0.gguf\", callbacks=callbacks, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chain = LLMChain(prompt=prompt, llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mileto/dev/lab/activeloop/01_langchain101/venv01/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `run` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. The rain falls from the sky and hits the ground, creating puddles or streams of water.\n",
      "2. The rain may soak into the soil, causing the ground to become wet.\n",
      "3. If the rain is heavy enough, it may cause flooding in low-lying areas.\n",
      "4. The rain may also cause erosion, as the force of the falling water can break down soil and rock formations.\n",
      "5. The rain may also create mud or slush on roads and other surfaces, making driving more difficult."
     ]
    },
    {
     "data": {
      "text/plain": [
       "'1. The rain falls from the sky and hits the ground, creating puddles or streams of water.\\n2. The rain may soak into the soil, causing the ground to become wet.\\n3. If the rain is heavy enough, it may cause flooding in low-lying areas.\\n4. The rain may also cause erosion, as the force of the falling water can break down soil and rock formations.\\n5. The rain may also create mud or slush on roads and other surfaces, making driving more difficult.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"What happens when it rains somewhere?\"\n",
    "llm_chain.run(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Let's answer in two sentence while being funny.\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chain = LLMChain(prompt=prompt, llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "When it rains, the plants grow and the animals get wet."
     ]
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "When it rains, the plants grow and the animals get wet."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"What happens when it rains somewhere?\"\n",
    "from IPython.display import Markdown\n",
    "response = llm_chain.run(question)\n",
    "Markdown(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
